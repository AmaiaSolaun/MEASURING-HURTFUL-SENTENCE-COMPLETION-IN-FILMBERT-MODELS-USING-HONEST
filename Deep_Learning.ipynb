{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmaiaSolaun/MEASURING-HURTFUL-SENTENCE-COMPLETION-IN-FILMBERT-MODELS-USING-HONEST/blob/main/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING THE MODEL"
      ],
      "metadata": {
        "id": "UCEySoYld1_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ5OQ6bNJqJA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV6I3rYk_OXa"
      },
      "outputs": [],
      "source": [
        "!unzip  \"/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/training.zip\" -d \"/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/\"\n",
        "!unzip  \"/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/test.zip\" -d \"/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/\"\n",
        "!unzip  \"/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/dev.zip\" -d \"/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCEllCSXJzks"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iiUJzoWGCLf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7ANVpw4GnTS"
      },
      "source": [
        "Loading the corpus. In our case we are going to be using the OpenSubtitle corpus.\n",
        "Since this corpus has a since of XXXX, we have reduced the size to XXXX in order to make it lighter.\n",
        "\n",
        "We are going to open and load the text in the corpus by sentences ending in a puntuaction mark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a4-E7c3K-ne"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv ('/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/training.csv').dropna()\n",
        "test = pd.read_csv ('/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/test.csv').dropna()\n",
        "dev = pd.read_csv ('/content/drive/MyDrive/Colab Notebooks/Deep_learning_project/dev.csv').dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul9eI43oCf1h"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "train = Dataset.from_pandas(train)\n",
        "dev = Dataset.from_pandas(dev)\n",
        "test = Dataset.from_pandas(test)\n",
        "\n",
        "dataset = DatasetDict()\n",
        "dataset['train'] = train\n",
        "dataset['validation'] = dev\n",
        "dataset['test'] = test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rejciFkXD34M",
        "outputId": "8b455950-3772-4f86-e39a-16cb9111d115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Unnamed: 0', 'text', '__index_level_0__'],\n",
            "        num_rows: 4586808\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['Unnamed: 0', 'text', '__index_level_0__'],\n",
            "        num_rows: 654993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Unnamed: 0', 'text', '__index_level_0__'],\n",
            "        num_rows: 1310319\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdBsRbVDECi1"
      },
      "outputs": [],
      "source": [
        "sample = dataset[\"train\"].shuffle(seed=42).select(range(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn95Sr3BEMQu",
        "outputId": "8d36b85c-2cd4-4b4d-c10a-4d0567d76357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Row id: 52408'\n",
            "Text: Sir?'\n",
            "\n",
            "Row id: 4482362'\n",
            "Text: Sure, the moon shone all night'\n",
            "\n",
            "Row id: 1974417'\n",
            "Text: You know, Greek women have the whitest skin in the world.'\n"
          ]
        }
      ],
      "source": [
        "for row in sample:\n",
        "    print(f\"\\nRow id: {row['Unnamed: 0']}'\")\n",
        "    print(f\"Text: {row['text']}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNnGxaqMEduE"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"text\"])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=[\"Unnamed: 0\", \"text\", \"__index_level_0__\"]\n",
        ")\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTDolFDbGd8K",
        "outputId": "7e662e1d-262e-443b-ad2d-f8aa490eca33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVqybZIR_tG_"
      },
      "outputs": [],
      "source": [
        "chunk_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2aMp4ukG9BR"
      },
      "outputs": [],
      "source": [
        "tokenized_samples = tokenized_dataset[\"train\"][:100]\n",
        "\n",
        "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
        "    print(f\"'>>> Sentence {idx} length: {len(sample)}'\")\n",
        "\n",
        "print(tokenized_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ulDOIpHHTA",
        "outputId": "b6965beb-28e7-46ab-ad9f-ec012eb6aefb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'>>> Concatenated texts length: 859'\n"
          ]
        }
      ],
      "source": [
        "concatenated_examples = {\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}\n",
        "total_length = len(concatenated_examples[\"input_ids\"])\n",
        "print(f\"'>>> Concatenated texts length: {total_length}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPPYRfRTHMSy"
      },
      "outputs": [],
      "source": [
        "chunks = {\n",
        "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "    for k, t in concatenated_examples.items()\n",
        "}\n",
        "\n",
        "for chunk in chunks[\"input_ids\"]:\n",
        "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X6sOyeoTbdQ"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate the texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create a new labels column\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbeEn5aqTuDs"
      },
      "outputs": [],
      "source": [
        "mlm_film_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
        "mlm_film_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9zZwLaC8faC"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(mlm_film_dataset[\"train\"][1][\"input_ids\"])\n",
        "tokenizer.decode(mlm_film_dataset[\"train\"][1][\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HydxQqX18lhU"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOJWYQGI8tCy"
      },
      "outputs": [],
      "source": [
        "samples = [mlm_film_dataset[\"train\"][i] for i in range(2)]\n",
        "for sample in samples:\n",
        "    _ = sample.pop(\"word_ids\")\n",
        "\n",
        "for chunk in data_collator(samples)[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx_2dEbH813Y"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from transformers.data.data_collator import tf_default_data_collator\n",
        "\n",
        "wwm_probability = 0.2\n",
        "\n",
        "\n",
        "def whole_word_masking_data_collator(features):\n",
        "    for feature in features:\n",
        "        word_ids = feature.pop(\"word_ids\")\n",
        "\n",
        "        # Create a map between words and corresponding token indices\n",
        "        mapping = collections.defaultdict(list)\n",
        "        current_word_index = -1\n",
        "        current_word = None\n",
        "        for idx, word_id in enumerate(word_ids):\n",
        "            if word_id is not None:\n",
        "                if word_id != current_word:\n",
        "                    current_word = word_id\n",
        "                    current_word_index += 1\n",
        "                mapping[current_word_index].append(idx)\n",
        "\n",
        "        # Randomly mask words\n",
        "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
        "        input_ids = feature[\"input_ids\"]\n",
        "        labels = feature[\"labels\"]\n",
        "        new_labels = [-100] * len(labels)\n",
        "        for word_id in np.where(mask)[0]:\n",
        "            word_id = word_id.item()\n",
        "            for idx in mapping[word_id]:\n",
        "                new_labels[idx] = labels[idx]\n",
        "                input_ids[idx] = tokenizer.mask_token_id\n",
        "        feature[\"labels\"] = new_labels\n",
        "\n",
        "    return tf_default_data_collator(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrf4yxPQ9I7t"
      },
      "outputs": [],
      "source": [
        "samples = [mlm_film_dataset[\"train\"][i] for i in range(2)]\n",
        "batch = whole_word_masking_data_collator(samples)\n",
        "\n",
        "for chunk in batch[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks3R6V3B9ilY"
      },
      "outputs": [],
      "source": [
        "train_size = 20000\n",
        "test_size = int(0.2 * train_size)\n",
        "\n",
        "downsampled_dataset = mlm_film_dataset[\"train\"].train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4vV1KuUn8x_"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgb3E0VceY8D"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"film{train_size}{model_name}\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-2,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=4,\n",
        "    push_to_hub=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEnXUqiLeewu"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=downsampled_dataset[\"train\"],\n",
        "    eval_dataset=downsampled_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx-ZbsEvehxg"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19wYL1Bqeh8O"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O42pDNmeul7"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-ELeFGVe0ft"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dISLgICRB0QA"
      },
      "source": [
        "## HONEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F4BnCL5EI52"
      },
      "outputs": [],
      "source": [
        "! pip install honest\n",
        "! pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNcApkx8EOcG"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from honest import honest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EnUTt8jCHR7"
      },
      "outputs": [],
      "source": [
        "name_model = \"AmaiaSolaun/film20000distilbert-base-uncased\"\n",
        "lang = \"en\"\n",
        "k = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO79GDMjBtkW"
      },
      "outputs": [],
      "source": [
        "evaluator = honest.HonestEvaluator(lang)\n",
        "masked_templates = evaluator.templates(data_set=\"binary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9W2gSRaB1bw"
      },
      "outputs": [],
      "source": [
        "masked_templates[\"the girl dreams of being a [M].\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ca6_tKVB2M8"
      },
      "outputs": [],
      "source": [
        "def get_completion(sentence, template):\n",
        "  start = sentence.find(template[:-4])+len(template[:-4])\n",
        "  if tokenizer.eos_token != None and tokenizer.eos_token in sentence:\n",
        "    end = sentence.find(tokenizer.eos_token)\n",
        "  elif tokenizer.sep_token != None and tokenizer.sep_token in sentence:\n",
        "    end = sentence.find(tokenizer.sep_token)\n",
        "  else:\n",
        "    end = len(sentence)-1\n",
        "\n",
        "  return sentence[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jcSrXj-CB4Hd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel , AutoModelForMaskedLM\n",
        "# Load BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(name_model, use_fast=True)\n",
        "model = AutoModelForMaskedLM.from_pretrained(name_model)\n",
        "\n",
        "# Define nlp_fill pipeline\n",
        "nlp_fill = pipeline('fill-mask', model=model, tokenizer=tokenizer, top_k=k)\n",
        "\n",
        "print(\"FILL EXAMPLE:\",nlp_fill('all women likes to [M].'.replace('[M]',tokenizer.mask_token)))\n",
        "\n",
        "\n",
        "# Fill templates (please check if the filled words contain any special character)\n",
        "filled_templates = [[get_completion(fill['sequence'],masked_sentence) for fill in nlp_fill(masked_sentence.replace('[M]',tokenizer.mask_token))] for masked_sentence in masked_templates.keys()]\n",
        "\n",
        "honest_score, honest_df = evaluator.honest_dataframe(filled_templates, masked_templates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KByduEXHqi6y",
        "outputId": "f6a2d028-44ea-4695-ae80-1d6b19125ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.03943370165745856\n"
          ]
        }
      ],
      "source": [
        "print(honest_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoT9GjaKEmZq"
      },
      "outputs": [],
      "source": [
        "display(honest_df)\n",
        "honest_df['honest_score'] = honest_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRGmZ1hNEn6h"
      },
      "outputs": [],
      "source": [
        "honest_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDTi64wMEqa6"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_y7TS4CEsTw"
      },
      "outputs": [],
      "source": [
        "# Initialize selected lexicon categories\n",
        "hurtlex_cat = ['animals', 'crime and immoral behavior', 'derogatory words', 'female genitalia', 'male genitalia', 'homosexuality', 'potential negative connotations', 'professions and occupations', 'prostitution', 'social and economic disadvantage']\n",
        "hurtlex_cat_names = ['an', 're', 'cds', 'asf', 'asm', 'om', 'qas', 'pa', 'pr', 'is']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XRP243YEumz"
      },
      "outputs": [],
      "source": [
        "df_identity = honest_df.groupby('category')[hurtlex_cat_names].sum()\n",
        "df_count = honest_df.groupby('category')[hurtlex_cat_names].count()*k\n",
        "df_perc = df_identity/df_count*100\n",
        "display(df_perc)\n",
        "\n",
        "plots = [go.Scatterpolar(r=df_perc.loc[i], theta=hurtlex_cat_names, fill='toself',\n",
        "                         name=i) for i, row in df_perc.iterrows()]\n",
        "\n",
        "fig = go.Figure(\n",
        "    data=plots,\n",
        "    layout=go.Layout(\n",
        "        polar={'radialaxis': {'visible': True}}\n",
        "    )\n",
        ")\n",
        "\n",
        "fig"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}